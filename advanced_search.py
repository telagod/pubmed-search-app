#!/usr/bin/env python3
"""
é«˜çº§PubMedæœç´¢å¼•æ“
==================
æ”¯æŒçµæ´»é…ç½®çš„æ–‡çŒ®æ£€ç´¢ç³»ç»Ÿ

ä½œè€…: KOOI Research Assistant
æ—¥æœŸ: 2025-11-10
"""

from typing import List, Optional, Dict, Any
from pathlib import Path
from datetime import datetime
from Bio import Entrez
import logging

from config_manager import ConfigManager, SearchParams, PubMedConfig

# ä»v2å¯¼å…¥ç°æœ‰çš„è§£æå™¨å’Œæ•°æ®æ¨¡å‹
import sys
sys.path.insert(0, str(Path(__file__).parent))

try:
    from pubmed_search_v2 import (
        PaperParser, Paper, PaperDatabase,
        FileExporter, setup_logging
    )
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥pubmed_search_v2æ¨¡å—")
    PaperParser = None
    Paper = None


class AdvancedPubMedSearchEngine:
    """é«˜çº§PubMedæœç´¢å¼•æ“"""

    def __init__(self, config: Optional[PubMedConfig] = None,
                 logger: Optional[logging.Logger] = None):
        """
        åˆå§‹åŒ–æœç´¢å¼•æ“

        Args:
            config: PubMedé…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ConfigManageråŠ è½½
            logger: æ—¥å¿—è®°å½•å™¨
        """
        self.config_manager = ConfigManager()

        if config is None:
            self.config = self.config_manager.get_pubmed_config()
        else:
            self.config = config

        self.logger = logger or logging.getLogger(__name__)

        # è®¾ç½®Entrez
        if self.config.email:
            Entrez.email = self.config.email
        if self.config.api_key:
            Entrez.api_key = self.config.api_key

        self.parser = PaperParser(self.logger) if PaperParser else None

    def validate_config(self) -> tuple[bool, str]:
        """
        éªŒè¯é…ç½®

        Returns:
            (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯)
        """
        if not self.config.email:
            return False, "æœªé…ç½®é‚®ç®±åœ°å€"

        if not self.config.api_key:
            return False, "æœªé…ç½®APIå¯†é’¥"

        if '@' not in self.config.email:
            return False, "é‚®ç®±æ ¼å¼æ— æ•ˆ"

        return True, ""

    def search(self, search_params: SearchParams) -> tuple[List[str], int]:
        """
        æœç´¢æ–‡çŒ®ID

        Args:
            search_params: æœç´¢å‚æ•°

        Returns:
            (IDåˆ—è¡¨, æ€»æ•°é‡)
        """
        # éªŒè¯é…ç½®
        valid, error_msg = self.validate_config()
        if not valid:
            self.logger.error(f"é…ç½®éªŒè¯å¤±è´¥: {error_msg}")
            raise ValueError(f"é…ç½®éªŒè¯å¤±è´¥: {error_msg}")

        self.logger.info(f"å¼€å§‹æœç´¢: {search_params.name}")
        self.logger.info(f"æŸ¥è¯¢: {search_params.query}")

        try:
            # æ„å»ºæœç´¢å‚æ•°
            esearch_params = search_params.to_esearch_params()

            handle = Entrez.esearch(**esearch_params)
            results = Entrez.read(handle)
            handle.close()

            id_list = results["IdList"]
            count = int(results["Count"])

            self.logger.info(f"âœ… æ‰¾åˆ° {count} ç¯‡æ–‡çŒ®ï¼Œè·å–å‰ {len(id_list)} ç¯‡ID")
            return id_list, count

        except Exception as e:
            self.logger.error(f"âŒ æœç´¢å¤±è´¥: {e}")
            return [], 0

    def fetch_details(self, id_list: List[str],
                     search_strategy: str = "",
                     batch_size: Optional[int] = None) -> List[Paper]:
        """
        æ‰¹é‡è·å–æ–‡çŒ®è¯¦æƒ…

        Args:
            id_list: æ–‡çŒ®IDåˆ—è¡¨
            search_strategy: æœç´¢ç­–ç•¥åç§°
            batch_size: æ‰¹æ¬¡å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®å€¼

        Returns:
            æ–‡çŒ®åˆ—è¡¨
        """
        if not self.parser:
            self.logger.error("è§£æå™¨æœªåˆå§‹åŒ–")
            return []

        papers = []
        total = len(id_list)

        if batch_size is None:
            batch_size = self.config.batch_size

        for i in range(0, total, batch_size):
            batch_ids = id_list[i:i+batch_size]
            batch_num = i // batch_size + 1
            total_batches = (total + batch_size - 1) // batch_size

            self.logger.info(f"ğŸ“– è·å–ç¬¬ {batch_num}/{total_batches} æ‰¹ ({len(batch_ids)} ç¯‡)...")

            try:
                handle = Entrez.efetch(
                    db="pubmed",
                    id=batch_ids,
                    rettype="xml",
                    retmode="xml"
                )

                records = Entrez.read(handle)
                handle.close()

                # è§£ææ¯ç¯‡æ–‡çŒ®
                for record in records.get('PubmedArticle', []):
                    paper = self.parser.parse_paper(record, search_strategy)
                    if paper:
                        papers.append(paper)

                self.logger.info(f"âœ“ æ‰¹æ¬¡ {batch_num} å®Œæˆï¼Œç´¯è®¡è§£æ {len(papers)} ç¯‡")

            except Exception as e:
                self.logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} å¤±è´¥: {e}")
                continue

        success_rate = len(papers) / total * 100 if total > 0 else 0
        self.logger.info(f"ğŸ¯ æ€»å…±æˆåŠŸ {len(papers)}/{total} ç¯‡ ({success_rate:.1f}%)")

        return papers

    def execute_search(self, search_params: SearchParams,
                      db_path: Optional[Path] = None,
                      export_dir: Optional[Path] = None,
                      save_to_db: bool = True,
                      export_formats: List[str] = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„æœç´¢æµç¨‹

        Args:
            search_params: æœç´¢å‚æ•°
            db_path: æ•°æ®åº“è·¯å¾„
            export_dir: å¯¼å‡ºç›®å½•
            save_to_db: æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“
            export_formats: å¯¼å‡ºæ ¼å¼åˆ—è¡¨ ['json', 'md', 'csv']

        Returns:
            æœç´¢ç»“æœå­—å…¸
        """
        if export_formats is None:
            export_formats = ['json', 'md', 'csv']

        # æœç´¢ID
        id_list, total_count = self.search(search_params)

        if not id_list:
            self.logger.warning(f"âš ï¸  æœªæ‰¾åˆ°æ–‡çŒ®: {search_params.query}")
            return {
                'success': False,
                'error': 'æœªæ‰¾åˆ°æ–‡çŒ®',
                'total_count': 0,
                'fetched_count': 0
            }

        # è·å–è¯¦æƒ…
        papers = self.fetch_details(id_list, search_strategy=search_params.name)

        if not papers:
            self.logger.warning(f"âš ï¸  æœªæˆåŠŸè§£æä»»ä½•æ–‡çŒ®")
            return {
                'success': False,
                'error': 'è§£æå¤±è´¥',
                'total_count': total_count,
                'fetched_count': 0
            }

        # ä¿å­˜åˆ°æ•°æ®åº“
        if save_to_db and db_path and PaperDatabase:
            db = PaperDatabase(db_path, self.logger)
            db.save_papers(papers)
            db.save_search_history(
                search_params.name,
                search_params.query,
                total_count,
                len(papers)
            )
            db.close()

        # å¯¼å‡ºæ–‡ä»¶
        exported_files = {}
        if export_dir and FileExporter:
            export_dir.mkdir(exist_ok=True)
            exporter = FileExporter(self.logger)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_name = "".join(c if c.isalnum() or c in ('-', '_') else '_'
                               for c in search_params.name)
            base_name = f"{safe_name}_{timestamp}"

            if 'json' in export_formats:
                json_file = export_dir / f"{base_name}.json"
                exporter.export_json(papers, json_file, search_params.query)
                exported_files['json'] = str(json_file)

            if 'md' in export_formats:
                md_file = export_dir / f"{base_name}.md"
                exporter.export_markdown(papers, md_file, search_params.query)
                exported_files['md'] = str(md_file)

            if 'csv' in export_formats:
                csv_file = export_dir / f"{base_name}.csv"
                exporter.export_csv(papers, csv_file)
                exported_files['csv'] = str(csv_file)

        # æ·»åŠ åˆ°æœç´¢å†å²
        self.config_manager.add_search_to_history(
            search_params,
            len(id_list),
            len(papers)
        )

        return {
            'success': True,
            'total_count': total_count,
            'fetched_count': len(papers),
            'success_rate': f"{len(papers)/len(id_list)*100:.1f}%",
            'papers': papers,
            'exported_files': exported_files
        }

    def build_query(self, keywords: List[str],
                   logic: str = "AND",
                   filters: Optional[Dict[str, Any]] = None) -> str:
        """
        æ„å»ºPubMedæŸ¥è¯¢å­—ç¬¦ä¸²

        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            logic: é€»è¾‘è¿ç®—ç¬¦ (AND/OR/NOT)
            filters: å…¶ä»–è¿‡æ»¤å™¨ {'journal': 'Nature', 'author': 'Smith'}

        Returns:
            æŸ¥è¯¢å­—ç¬¦ä¸²
        """
        # åŸºç¡€å…³é”®è¯æŸ¥è¯¢
        if not keywords:
            return ""

        query_parts = [f" {logic} ".join(keywords)]

        # æ·»åŠ è¿‡æ»¤å™¨
        if filters:
            for field, value in filters.items():
                if value:
                    query_parts.append(f"{value}[{field}]")

        return " AND ".join(query_parts)

    @staticmethod
    def get_field_tags() -> Dict[str, str]:
        """
        è·å–PubMedå­—æ®µæ ‡ç­¾

        Returns:
            å­—æ®µæ ‡ç­¾å­—å…¸
        """
        return {
            'Title': 'Title',
            'Abstract': 'Abstract',
            'Author': 'Author',
            'Journal': 'Journal',
            'Affiliation': 'Affiliation',
            'MeSH Terms': 'MeSH Terms',
            'All Fields': 'All Fields',
            'Publication Date': 'Publication Date',
            'Publication Type': 'Publication Type'
        }


def create_search_engine(config: Optional[PubMedConfig] = None) -> AdvancedPubMedSearchEngine:
    """
    åˆ›å»ºæœç´¢å¼•æ“å®ä¾‹

    Args:
        config: PubMedé…ç½®

    Returns:
        æœç´¢å¼•æ“å®ä¾‹
    """
    # è®¾ç½®æ—¥å¿—
    logger = logging.getLogger("PubMedSearch")
    if not logger.handlers:
        logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)

    return AdvancedPubMedSearchEngine(config, logger)


if __name__ == "__main__":
    # æµ‹è¯•æœç´¢å¼•æ“
    print("=== æµ‹è¯•é«˜çº§PubMedæœç´¢å¼•æ“ ===\n")

    # åˆ›å»ºæœç´¢å¼•æ“
    engine = create_search_engine()

    # éªŒè¯é…ç½®
    valid, error_msg = engine.validate_config()
    print(f"é…ç½®éªŒè¯: {'âœ… æœ‰æ•ˆ' if valid else f'âŒ {error_msg}'}\n")

    if valid:
        # æµ‹è¯•æŸ¥è¯¢æ„å»º
        query = engine.build_query(
            keywords=['TP53', 'cancer'],
            logic='AND'
        )
        print(f"æ„å»ºçš„æŸ¥è¯¢: {query}\n")

        # åˆ›å»ºæœç´¢å‚æ•°
        search_params = SearchParams(
            query=query,
            name="Test Search",
            max_results=5
        )

        # æ‰§è¡Œæœç´¢
        print("æ‰§è¡Œæµ‹è¯•æœç´¢ï¼ˆä»…å‰5ç¯‡ï¼‰...\n")
        result = engine.execute_search(
            search_params,
            save_to_db=False,
            export_formats=[]
        )

        print(f"\næœç´¢ç»“æœ:")
        print(f"  æ€»æ•°: {result['total_count']}")
        print(f"  è·å–: {result['fetched_count']}")
        print(f"  æˆåŠŸç‡: {result.get('success_rate', 'N/A')}")
