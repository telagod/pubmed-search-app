#!/usr/bin/env python3
"""
é€šç”¨æ–‡çŒ®æ£€ç´¢ç³»ç»Ÿ - ä¼˜åŒ–ç‰ˆ v2.0
===============================
ç‰¹æ€§:
- ä½¿ç”¨dataclassè¿›è¡Œæ•°æ®å»ºæ¨¡
- å¥å£®çš„æ–‡çŒ®è§£æï¼ˆå¤„ç†å¤šç§æ ¼å¼ï¼‰
- SQLiteæ•°æ®åº“æ”¯æŒ
- å¤šæ ¼å¼å¯¼å‡º(JSON/MD/CSV)
- å®Œæ•´çš„æ—¥å¿—è®°å½•
- ä¸ºStreamlitå¯è§†åŒ–å‡†å¤‡

ä½œè€…: KOOI Research Assistant
æ—¥æœŸ: 2025-11-10
"""

from dataclasses import dataclass, field, asdict
from typing import List, Optional, Dict, Any
from pathlib import Path
from datetime import datetime
from Bio import Entrez
import json
import sqlite3
from enum import Enum
import logging
import csv


# ==================== é…ç½®æ—¥å¿— ====================
def setup_logging(log_dir: Path):
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_dir.mkdir(exist_ok=True)
    log_file = log_dir / f"pubmed_search_{datetime.now().strftime('%Y%m%d')}.log"

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)


# ==================== æ•°æ®æ¨¡å‹ ====================
class SearchStrategy(Enum):
    """æ£€ç´¢ç­–ç•¥æšä¸¾ï¼ˆç¤ºä¾‹ï¼‰"""
    EXAMPLE_ONCO = "TP53 AND (cancer OR tumor)"
    EXAMPLE_NEURO = "Alzheimer AND amyloid"
    EXAMPLE_IMMUNE = "T cell AND cytokine"
    EXAMPLE_METHOD = "single-cell AND RNA-seq"


@dataclass
class Author:
    """ä½œè€…ä¿¡æ¯"""
    last_name: str
    first_name: str = ""
    initials: str = ""
    affiliation: str = ""

    def __str__(self):
        if self.initials:
            return f"{self.last_name} {self.initials}"
        return f"{self.last_name} {self.first_name}"


@dataclass
class PubDate:
    """å‘è¡¨æ—¥æœŸ"""
    year: str = ""
    month: str = ""
    day: str = ""

    def __str__(self):
        parts = [p for p in [self.year, self.month, self.day] if p]
        return "-".join(parts) if parts else "Unknown"

    @property
    def is_complete(self) -> bool:
        return bool(self.year)


@dataclass
class Paper:
    """æ–‡çŒ®ä¿¡æ¯å®Œæ•´æ¨¡å‹"""
    pmid: str
    title: str
    abstract: str = ""
    journal: str = ""
    pub_date: PubDate = field(default_factory=PubDate)
    authors: List[Author] = field(default_factory=list)
    keywords: List[str] = field(default_factory=list)
    doi: str = ""
    mesh_terms: List[str] = field(default_factory=list)
    search_strategy: str = ""
    fetch_date: str = field(default_factory=lambda: datetime.now().isoformat())

    @property
    def author_string(self) -> str:
        """è·å–ä½œè€…å­—ç¬¦ä¸²ï¼ˆå‰3ä½+et alï¼‰"""
        if not self.authors:
            return "No authors"

        authors_str = ", ".join(str(a) for a in self.authors[:3])
        if len(self.authors) > 3:
            authors_str += f" et al."
        return authors_str

    @property
    def pubmed_url(self) -> str:
        """PubMedé“¾æ¥"""
        return f"https://pubmed.ncbi.nlm.nih.gov/{self.pmid}/"

    @property
    def year(self) -> str:
        """å‘è¡¨å¹´ä»½"""
        return self.pub_date.year or "Unknown"

    @property
    def has_abstract(self) -> bool:
        """æ˜¯å¦æœ‰æ‘˜è¦"""
        return bool(self.abstract and len(self.abstract) > 50)

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸ï¼ˆç”¨äºJSONå¯¼å‡ºå’Œå¯è§†åŒ–ï¼‰"""
        return {
            'pmid': self.pmid,
            'title': self.title,
            'abstract': self.abstract,
            'journal': self.journal,
            'year': self.year,
            'pub_date': str(self.pub_date),
            'authors': [str(a) for a in self.authors],
            'author_string': self.author_string,
            'keywords': self.keywords,
            'mesh_terms': self.mesh_terms,
            'doi': self.doi,
            'pubmed_url': self.pubmed_url,
            'search_strategy': self.search_strategy,
            'fetch_date': self.fetch_date,
            'has_abstract': self.has_abstract
        }


# ==================== æ–‡çŒ®è§£æå™¨ ====================
class PaperParser:
    """å¥å£®çš„æ–‡çŒ®è§£æå™¨ - å¤„ç†PubMed XMLçš„å„ç§æ ¼å¼"""

    def __init__(self, logger):
        self.logger = logger

    def parse_abstract(self, article: Dict) -> str:
        """
        è§£ææ‘˜è¦ - å¤„ç†å¤šç§æ ¼å¼

        AbstractTextå¯èƒ½çš„æ ¼å¼:
        1. å­—ç¬¦ä¸²
        2. å­—ç¬¦ä¸²åˆ—è¡¨
        3. å¸¦Labelçš„ç»“æ„åŒ–å­—å…¸åˆ—è¡¨
        4. ç©º
        """
        if 'Abstract' not in article:
            return ""

        abstract_data = article['Abstract']
        if 'AbstractText' not in abstract_data:
            return ""

        abstract_text = abstract_data['AbstractText']

        # ç©ºå€¼æ£€æŸ¥
        if not abstract_text:
            return ""

        # å•ä¸ªå­—ç¬¦ä¸²
        if isinstance(abstract_text, str):
            return abstract_text.strip()

        # åˆ—è¡¨æ ¼å¼
        if isinstance(abstract_text, list):
            parts = []
            for item in abstract_text:
                if isinstance(item, str):
                    # ç®€å•å­—ç¬¦ä¸²
                    parts.append(item.strip())
                elif isinstance(item, dict):
                    # ç»“æ„åŒ–æ‘˜è¦ (å¦‚ {'@Label': 'BACKGROUND', '#text': '...'})
                    label = item.get('@Label', '')
                    text = item.get('#text', '') or str(item)
                    if label and text:
                        parts.append(f"**{label}**: {text}")
                    elif text:
                        parts.append(text)
                else:
                    # å…¶ä»–ç±»å‹ï¼Œè½¬å­—ç¬¦ä¸²
                    text = str(item).strip()
                    if text:
                        parts.append(text)

            return " ".join(parts)

        # å…¶ä»–æƒ…å†µï¼Œå°è¯•è½¬å­—ç¬¦ä¸²
        return str(abstract_text).strip()

    def parse_authors(self, article: Dict) -> List[Author]:
        """è§£æä½œè€…åˆ—è¡¨"""
        authors = []

        if 'AuthorList' not in article:
            return authors

        for author_data in article['AuthorList']:
            try:
                # è·³è¿‡é›†ä½“ä½œè€…
                if 'CollectiveName' in author_data:
                    self.logger.debug(f"è·³è¿‡é›†ä½“ä½œè€…: {author_data.get('CollectiveName')}")
                    continue

                # æå–ä½œè€…ä¿¡æ¯
                last_name = author_data.get('LastName', '')
                if not last_name:
                    continue

                # æå–æœºæ„ä¿¡æ¯
                affiliation = ""
                if 'AffiliationInfo' in author_data:
                    aff_list = author_data['AffiliationInfo']
                    if aff_list and isinstance(aff_list, list):
                        affiliation = aff_list[0].get('Affiliation', '')

                author = Author(
                    last_name=last_name,
                    first_name=author_data.get('ForeName', ''),
                    initials=author_data.get('Initials', ''),
                    affiliation=affiliation
                )
                authors.append(author)

            except Exception as e:
                self.logger.warning(f"è§£æä½œè€…æ—¶å‡ºé”™: {e}")
                continue

        return authors

    def parse_pub_date(self, article: Dict) -> PubDate:
        """è§£æå‘è¡¨æ—¥æœŸ - å¤„ç†å¤šç§æ—¥æœŸæ ¼å¼"""
        pub_date = PubDate()

        try:
            journal = article.get('Journal', {})
            journal_issue = journal.get('JournalIssue', {})
            date_data = journal_issue.get('PubDate', {})

            if not date_data:
                return pub_date

            # ä¼˜å…ˆä½¿ç”¨æ ‡å‡†æ ¼å¼
            if 'Year' in date_data:
                pub_date.year = str(date_data['Year'])
                pub_date.month = str(date_data.get('Month', ''))
                pub_date.day = str(date_data.get('Day', ''))

            # å¤„ç†MedlineDate (å¦‚ "2020 Jan-Feb", "2020 Spring")
            elif 'MedlineDate' in date_data:
                medline_date = str(date_data['MedlineDate'])
                parts = medline_date.split()
                if parts:
                    # ç¬¬ä¸€éƒ¨åˆ†é€šå¸¸æ˜¯å¹´ä»½
                    pub_date.year = parts[0]
                    if len(parts) > 1:
                        # ç¬¬äºŒéƒ¨åˆ†å¯èƒ½æ˜¯æœˆä»½æˆ–å­£èŠ‚
                        pub_date.month = parts[1]

        except Exception as e:
            self.logger.warning(f"è§£ææ—¥æœŸæ—¶å‡ºé”™: {e}")

        return pub_date

    def parse_keywords(self, record: Dict) -> List[str]:
        """è§£æå…³é”®è¯"""
        keywords = []

        try:
            citation = record.get('MedlineCitation', {})

            if 'KeywordList' in citation:
                for keyword_list in citation['KeywordList']:
                    for kw in keyword_list:
                        # å¤„ç†ä¸åŒæ ¼å¼çš„å…³é”®è¯
                        if isinstance(kw, dict):
                            keyword_str = kw.get('#text', '') or str(kw)
                        else:
                            keyword_str = str(kw)

                        keyword_str = keyword_str.strip()
                        if keyword_str and keyword_str not in keywords:
                            keywords.append(keyword_str)

        except Exception as e:
            self.logger.warning(f"è§£æå…³é”®è¯æ—¶å‡ºé”™: {e}")

        return keywords

    def parse_mesh_terms(self, record: Dict) -> List[str]:
        """è§£æMeSHä¸»é¢˜è¯"""
        mesh_terms = []

        try:
            citation = record.get('MedlineCitation', {})

            if 'MeshHeadingList' in citation:
                for mesh_heading in citation['MeshHeadingList']:
                    if 'DescriptorName' in mesh_heading:
                        descriptor = mesh_heading['DescriptorName']

                        # æå–ä¸»é¢˜è¯æ–‡æœ¬
                        if isinstance(descriptor, dict):
                            term = descriptor.get('#text', '') or str(descriptor)
                        else:
                            term = str(descriptor)

                        term = term.strip()
                        if term and term not in mesh_terms:
                            mesh_terms.append(term)

        except Exception as e:
            self.logger.warning(f"è§£æMeSHæ—¶å‡ºé”™: {e}")

        return mesh_terms

    def parse_doi(self, record: Dict) -> str:
        """è§£æDOI"""
        try:
            # æ–¹æ³•1: ä»Articleçš„ELocationIDè·å–
            article = record.get('MedlineCitation', {}).get('Article', {})
            if 'ELocationID' in article:
                for elocation in article['ELocationID']:
                    if isinstance(elocation, dict):
                        if elocation.get('@EIdType') == 'doi':
                            return str(elocation.get('#text', ''))
                    else:
                        eloc_str = str(elocation)
                        if 'doi' in eloc_str.lower():
                            return eloc_str

            # æ–¹æ³•2: ä»PubmedDataçš„ArticleIdListè·å–
            pubmed_data = record.get('PubmedData', {})
            if 'ArticleIdList' in pubmed_data:
                for article_id in pubmed_data['ArticleIdList']:
                    if isinstance(article_id, dict):
                        if article_id.get('@IdType') == 'doi':
                            return str(article_id.get('#text', ''))

        except Exception as e:
            self.logger.warning(f"è§£æDOIæ—¶å‡ºé”™: {e}")

        return ""

    def parse_paper(self, record: Dict, search_strategy: str = "") -> Optional[Paper]:
        """è§£æå•ç¯‡æ–‡çŒ® - ä¸»å‡½æ•°"""
        try:
            citation = record.get('MedlineCitation', {})
            article = citation.get('Article', {})

            # å¿…éœ€å­—æ®µæ£€æŸ¥
            if 'PMID' not in citation:
                self.logger.warning("ç¼ºå°‘PMIDï¼Œè·³è¿‡æ­¤æ–‡çŒ®")
                return None

            pmid = str(citation['PMID'])

            if 'ArticleTitle' not in article:
                self.logger.warning(f"PMID {pmid} ç¼ºå°‘æ ‡é¢˜ï¼Œè·³è¿‡")
                return None

            # åˆ›å»ºPaperå¯¹è±¡
            paper = Paper(
                pmid=pmid,
                title=str(article['ArticleTitle']).strip(),
                abstract=self.parse_abstract(article),
                journal=article.get('Journal', {}).get('Title', ''),
                pub_date=self.parse_pub_date(article),
                authors=self.parse_authors(article),
                keywords=self.parse_keywords(record),
                mesh_terms=self.parse_mesh_terms(record),
                doi=self.parse_doi(record),
                search_strategy=search_strategy
            )

            self.logger.debug(f"æˆåŠŸè§£æ: PMID {pmid}")
            return paper

        except Exception as e:
            self.logger.error(f"è§£ææ–‡çŒ®æ—¶å‡ºé”™: {e}", exc_info=True)
            return None


# ==================== æ•°æ®åº“ç®¡ç† ====================
class PaperDatabase:
    """SQLiteæ•°æ®åº“ç®¡ç†"""

    def __init__(self, db_path: Path, logger):
        self.db_path = db_path
        self.logger = logger
        self.conn = None
        self._init_db()

    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“schema"""
        self.conn = sqlite3.connect(str(self.db_path))
        cursor = self.conn.cursor()

        # åˆ›å»ºpapersè¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS papers (
                pmid TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                abstract TEXT,
                journal TEXT,
                pub_year TEXT,
                pub_date TEXT,
                authors TEXT,
                keywords TEXT,
                mesh_terms TEXT,
                doi TEXT,
                search_strategy TEXT,
                fetch_date TEXT,
                pubmed_url TEXT,
                has_abstract INTEGER
            )
        ''')

        # åˆ›å»ºæœç´¢å†å²è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS search_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                strategy_name TEXT,
                query TEXT,
                total_count INTEGER,
                fetched_count INTEGER,
                success_rate REAL,
                search_date TEXT
            )
        ''')

        # åˆ›å»ºç´¢å¼•æå‡æŸ¥è¯¢æ€§èƒ½
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_pub_year ON papers(pub_year)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_strategy ON papers(search_strategy)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_journal ON papers(journal)')

        self.conn.commit()
        self.logger.info(f"æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {self.db_path}")

    def save_paper(self, paper: Paper):
        """ä¿å­˜å•ç¯‡æ–‡çŒ®"""
        cursor = self.conn.cursor()

        cursor.execute('''
            INSERT OR REPLACE INTO papers
            (pmid, title, abstract, journal, pub_year, pub_date,
             authors, keywords, mesh_terms, doi, search_strategy,
             fetch_date, pubmed_url, has_abstract)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            paper.pmid,
            paper.title,
            paper.abstract,
            paper.journal,
            paper.year,
            str(paper.pub_date),
            json.dumps([str(a) for a in paper.authors]),
            json.dumps(paper.keywords),
            json.dumps(paper.mesh_terms),
            paper.doi,
            paper.search_strategy,
            paper.fetch_date,
            paper.pubmed_url,
            1 if paper.has_abstract else 0
        ))

        self.conn.commit()

    def save_papers(self, papers: List[Paper]):
        """æ‰¹é‡ä¿å­˜æ–‡çŒ®"""
        for paper in papers:
            self.save_paper(paper)
        self.logger.info(f"ä¿å­˜äº† {len(papers)} ç¯‡æ–‡çŒ®åˆ°æ•°æ®åº“")

    def save_search_history(self, strategy_name: str, query: str,
                          total_count: int, fetched_count: int):
        """ä¿å­˜æœç´¢å†å²"""
        cursor = self.conn.cursor()

        success_rate = (fetched_count / total_count * 100) if total_count > 0 else 0

        cursor.execute('''
            INSERT INTO search_history
            (strategy_name, query, total_count, fetched_count, success_rate, search_date)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            strategy_name,
            query,
            total_count,
            fetched_count,
            success_rate,
            datetime.now().isoformat()
        ))

        self.conn.commit()

    def get_all_papers(self) -> List[Dict]:
        """è·å–æ‰€æœ‰æ–‡çŒ®ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰"""
        cursor = self.conn.cursor()
        cursor.execute('SELECT * FROM papers')

        columns = [desc[0] for desc in cursor.description]
        papers = []

        for row in cursor.fetchall():
            paper_dict = dict(zip(columns, row))
            # è§£æJSONå­—æ®µ
            paper_dict['authors'] = json.loads(paper_dict['authors'])
            paper_dict['keywords'] = json.loads(paper_dict['keywords'])
            paper_dict['mesh_terms'] = json.loads(paper_dict['mesh_terms'])
            papers.append(paper_dict)

        return papers

    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºDashboardï¼‰"""
        cursor = self.conn.cursor()

        stats = {}

        # æ€»æ–‡çŒ®æ•°
        cursor.execute('SELECT COUNT(*) FROM papers')
        stats['total_papers'] = cursor.fetchone()[0]

        # æŒ‰å¹´ä»½ç»Ÿè®¡
        cursor.execute('''
            SELECT pub_year, COUNT(*)
            FROM papers
            WHERE pub_year != '' AND pub_year != 'Unknown'
            GROUP BY pub_year
            ORDER BY pub_year DESC
        ''')
        stats['by_year'] = dict(cursor.fetchall())

        # æŒ‰æ£€ç´¢ç­–ç•¥ç»Ÿè®¡
        cursor.execute('''
            SELECT search_strategy, COUNT(*)
            FROM papers
            GROUP BY search_strategy
        ''')
        stats['by_strategy'] = dict(cursor.fetchall())

        # æœ‰æ‘˜è¦çš„æ–‡çŒ®æ•°
        cursor.execute('SELECT COUNT(*) FROM papers WHERE has_abstract = 1')
        stats['with_abstract'] = cursor.fetchone()[0]

        return stats

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()
            self.logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")


# ==================== PubMed APIæ¥å£ ====================
class PubMedAPI:
    """PubMed APIå°è£…"""

    def __init__(self, email: str, api_key: str, logger):
        self.email = email
        self.api_key = api_key
        self.logger = logger
        Entrez.email = email
        Entrez.api_key = api_key
        self.logger.info(f"PubMed APIåˆå§‹åŒ–: {email}")

    def search(self, query: str, max_results: int = 100) -> tuple:
        """æœç´¢æ–‡çŒ®ID"""
        self.logger.info(f"å¼€å§‹æœç´¢: {query}")

        try:
            handle = Entrez.esearch(
                db="pubmed",
                term=query,
                retmax=max_results,
                sort="relevance"
            )

            results = Entrez.read(handle)
            handle.close()

            id_list = results["IdList"]
            count = int(results["Count"])

            self.logger.info(f"âœ… æ‰¾åˆ° {count} ç¯‡æ–‡çŒ®ï¼Œè·å–å‰ {len(id_list)} ç¯‡ID")
            return id_list, count

        except Exception as e:
            self.logger.error(f"âŒ æœç´¢å¤±è´¥: {e}")
            return [], 0

    def fetch_details(self, id_list: List[str],
                     search_strategy: str = "",
                     batch_size: int = 20) -> List[Paper]:
        """æ‰¹é‡è·å–æ–‡çŒ®è¯¦æƒ…"""
        papers = []
        total = len(id_list)
        parser = PaperParser(self.logger)

        for i in range(0, total, batch_size):
            batch_ids = id_list[i:i+batch_size]
            batch_num = i // batch_size + 1
            total_batches = (total + batch_size - 1) // batch_size

            self.logger.info(f"ğŸ“– è·å–ç¬¬ {batch_num}/{total_batches} æ‰¹ ({len(batch_ids)} ç¯‡)...")

            try:
                handle = Entrez.efetch(
                    db="pubmed",
                    id=batch_ids,
                    rettype="xml",
                    retmode="xml"
                )

                records = Entrez.read(handle)
                handle.close()

                # è§£ææ¯ç¯‡æ–‡çŒ®
                for record in records.get('PubmedArticle', []):
                    paper = parser.parse_paper(record, search_strategy)
                    if paper:
                        papers.append(paper)

                self.logger.info(f"âœ“ æ‰¹æ¬¡ {batch_num} å®Œæˆï¼Œç´¯è®¡è§£æ {len(papers)} ç¯‡")

            except Exception as e:
                self.logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} å¤±è´¥: {e}")
                continue

        success_rate = len(papers) / total * 100 if total > 0 else 0
        self.logger.info(f"ğŸ¯ æ€»å…±æˆåŠŸ {len(papers)}/{total} ç¯‡ ({success_rate:.1f}%)")

        return papers


# ==================== æ–‡ä»¶å¯¼å‡ºå™¨ ====================
class FileExporter:
    """å¤šæ ¼å¼æ–‡ä»¶å¯¼å‡º"""

    def __init__(self, logger):
        self.logger = logger

    def export_json(self, papers: List[Paper], filepath: Path, query: str = ""):
        """å¯¼å‡ºä¸ºJSONï¼ˆç”¨äºç¨‹åºé—´æ•°æ®äº¤æ¢ï¼‰"""
        data = {
            'metadata': {
                'query': query,
                'export_date': datetime.now().isoformat(),
                'total_papers': len(papers),
                'format_version': '2.0'
            },
            'papers': [p.to_dict() for p in papers]
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        self.logger.info(f"ğŸ“„ å¯¼å‡ºJSON: {filepath} ({len(papers)} ç¯‡)")

    def export_markdown(self, papers: List[Paper], filepath: Path, query: str = ""):
        """å¯¼å‡ºä¸ºMarkdownï¼ˆç”¨äºé˜…è¯»å’Œæ–‡æ¡£ï¼‰"""
        with open(filepath, 'w', encoding='utf-8') as f:
            # å¤´éƒ¨ä¿¡æ¯
            f.write(f"# æ–‡çŒ®æ£€ç´¢ç»“æœ\n\n")
            f.write(f"**æ£€ç´¢æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**æ£€ç´¢ç­–ç•¥**: {query}\n")
            f.write(f"**æ–‡çŒ®æ•°é‡**: {len(papers)}\n\n")
            f.write("---\n\n")

            # æ–‡çŒ®åˆ—è¡¨
            for idx, paper in enumerate(papers, 1):
                f.write(f"## {idx}. {paper.title}\n\n")

                # åŸºæœ¬ä¿¡æ¯
                f.write(f"- **PMID**: [{paper.pmid}]({paper.pubmed_url})\n")
                if paper.doi:
                    f.write(f"- **DOI**: {paper.doi}\n")
                f.write(f"- **æœŸåˆŠ**: {paper.journal}\n")
                f.write(f"- **å‘è¡¨æ—¥æœŸ**: {paper.pub_date}\n")

                # ä½œè€…
                if paper.authors:
                    f.write(f"- **ä½œè€…**: {paper.author_string}\n")

                # å…³é”®è¯
                if paper.keywords:
                    kw_str = ", ".join(paper.keywords[:10])
                    if len(paper.keywords) > 10:
                        kw_str += f" (å…±{len(paper.keywords)}ä¸ª)"
                    f.write(f"- **å…³é”®è¯**: {kw_str}\n")

                # MeSHä¸»é¢˜è¯
                if paper.mesh_terms:
                    mesh_str = ", ".join(paper.mesh_terms[:8])
                    if len(paper.mesh_terms) > 8:
                        mesh_str += f" (å…±{len(paper.mesh_terms)}ä¸ª)"
                    f.write(f"- **MeSHä¸»é¢˜è¯**: {mesh_str}\n")

                # æ‘˜è¦
                if paper.abstract:
                    f.write(f"\n### æ‘˜è¦\n\n{paper.abstract}\n")

                f.write("\n---\n\n")

        self.logger.info(f"ğŸ“ å¯¼å‡ºMarkdown: {filepath} ({len(papers)} ç¯‡)")

    def export_csv(self, papers: List[Paper], filepath: Path):
        """å¯¼å‡ºä¸ºCSVï¼ˆç”¨äºExcelå’Œæ•°æ®åˆ†æï¼‰"""
        with open(filepath, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)

            # è¡¨å¤´
            writer.writerow([
                'PMID', 'æ ‡é¢˜', 'æœŸåˆŠ', 'å‘è¡¨å¹´ä»½', 'å‘è¡¨æ—¥æœŸ',
                'ä½œè€…', 'å…³é”®è¯æ•°é‡', 'å…³é”®è¯', 'MeSHä¸»é¢˜è¯æ•°é‡',
                'MeSHä¸»é¢˜è¯', 'DOI', 'æœ‰æ‘˜è¦', 'PubMedé“¾æ¥', 'æ‘˜è¦é¢„è§ˆ'
            ])

            # æ•°æ®è¡Œ
            for paper in papers:
                writer.writerow([
                    paper.pmid,
                    paper.title,
                    paper.journal,
                    paper.year,
                    str(paper.pub_date),
                    paper.author_string,
                    len(paper.keywords),
                    "; ".join(paper.keywords),
                    len(paper.mesh_terms),
                    "; ".join(paper.mesh_terms),
                    paper.doi,
                    "æ˜¯" if paper.has_abstract else "å¦",
                    paper.pubmed_url,
                    (paper.abstract[:200] + '...') if len(paper.abstract) > 200 else paper.abstract
                ])

        self.logger.info(f"ğŸ“Š å¯¼å‡ºCSV: {filepath} ({len(papers)} ç¯‡)")


# ==================== é…ç½®ç®¡ç† ====================
def load_env(env_path: Path) -> Dict[str, str]:
    """ä».envæ–‡ä»¶åŠ è½½é…ç½®"""
    config = {}

    if not env_path.exists():
        print(f"âš ï¸  .envæ–‡ä»¶ä¸å­˜åœ¨: {env_path}")
        return config

    with open(env_path) as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith("#") and ':' in line:
                key, value = line.split(":", 1)
                config[key.strip()] = value.strip()

    return config


# ==================== ä¸»å‡½æ•° ====================
def main():
    """ä¸»å‡½æ•° - æ‰§è¡Œå®Œæ•´æ£€ç´¢æµç¨‹"""
    print("=" * 70)
    print("ğŸ” é€šç”¨æ–‡çŒ®æ£€ç´¢ç³»ç»Ÿ v2.0 - ä¼˜åŒ–ç‰ˆ")
    print("=" * 70)
    print()

    # 1. åˆå§‹åŒ–
    script_dir = Path(__file__).parent
    output_dir = script_dir / "results"
    output_dir.mkdir(exist_ok=True)

    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(output_dir)

    # 2. åŠ è½½é…ç½®
    env_path = script_dir.parent / ".env"
    config = load_env(env_path)

    email = config.get('pubmed_email')
    api_key = config.get('api_key')

    if not email or not api_key:
        logger.error("âŒ é…ç½®é”™è¯¯: æœªæ‰¾åˆ°é‚®ç®±æˆ–APIå¯†é’¥")
        print("è¯·æ£€æŸ¥ .env æ–‡ä»¶é…ç½®")
        return

    # 3. åˆå§‹åŒ–ç»„ä»¶
    api = PubMedAPI(email, api_key, logger)
    db_path = output_dir / "bmal1_papers.db"
    db = PaperDatabase(db_path, logger)
    exporter = FileExporter(logger)

    # 4. æ‰§è¡Œæ£€ç´¢
    all_results = {}

    for strategy in SearchStrategy:
        print(f"\n{'='*70}")
        print(f"ğŸ“š æ£€ç´¢ç­–ç•¥: {strategy.name}")
        print(f"ğŸ” æŸ¥è¯¢è¯­å¥: {strategy.value}")
        print(f"{'='*70}\n")

        # æœç´¢ID
        id_list, total_count = api.search(strategy.value, max_results=50)

        if not id_list:
            logger.warning(f"âš ï¸  æœªæ‰¾åˆ°æ–‡çŒ®: {strategy.value}")
            continue

        # è·å–è¯¦æƒ…
        papers = api.fetch_details(id_list, search_strategy=strategy.name)

        if not papers:
            logger.warning(f"âš ï¸  æœªæˆåŠŸè§£æä»»ä½•æ–‡çŒ®")
            continue

        # ä¿å­˜åˆ°æ•°æ®åº“
        db.save_papers(papers)
        db.save_search_history(strategy.name, strategy.value, total_count, len(papers))

        # å¯¼å‡ºæ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = f"bmal1_{strategy.name.lower()}_{timestamp}"

        json_file = output_dir / f"{base_name}.json"
        md_file = output_dir / f"{base_name}.md"
        csv_file = output_dir / f"{base_name}.csv"

        exporter.export_json(papers, json_file, strategy.value)
        exporter.export_markdown(papers, md_file, strategy.value)
        exporter.export_csv(papers, csv_file)

        # è®°å½•ç»“æœ
        all_results[strategy.name] = {
            'query': strategy.value,
            'total_count': total_count,
            'fetched_count': len(papers),
            'success_rate': f"{len(papers)/len(id_list)*100:.1f}%",
            'files': {
                'json': json_file.name,
                'markdown': md_file.name,
                'csv': csv_file.name
            }
        }

        logger.info(f"âœ… {strategy.name} å®Œæˆ\n")

    # 5. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    summary_file = output_dir / f"search_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

    summary_data = {
        'metadata': {
            'search_date': datetime.now().isoformat(),
            'database_path': str(db_path),
            'version': '2.0'
        },
        'statistics': db.get_statistics(),
        'search_results': all_results
    }

    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, ensure_ascii=False, indent=2)

    logger.info(f"ğŸ“Š æ£€ç´¢æ‘˜è¦: {summary_file}")

    # 6. æ¸…ç†
    db.close()

    # 7. è¾“å‡ºæ€»ç»“
    print(f"\n{'='*70}")
    print("âœ… æ‰€æœ‰æ£€ç´¢ä»»åŠ¡å®Œæˆï¼")
    print(f"{'='*70}")
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    stats = summary_data['statistics']
    print(f"  - æ€»æ–‡çŒ®æ•°: {stats['total_papers']}")
    print(f"  - æœ‰æ‘˜è¦: {stats['with_abstract']}")
    print(f"  - æ•°æ®åº“: {db_path}")
    print(f"  - è¾“å‡ºç›®å½•: {output_dir}")
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥: è¿è¡Œ Streamlit å¯è§†åŒ–ç•Œé¢æŸ¥çœ‹ç»“æœ")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()
